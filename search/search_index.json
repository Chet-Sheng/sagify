{
    "docs": [
        {
            "location": "/",
            "text": "sagify\n\n\n\n\nA command-line utility to train and deploy Machine Learning/Deep Learning models on \nAWS SageMaker\n in a few simple steps!\n\n\n\n\nInstallation\n\n\nPrerequisites\n\n\nsagify requires the following:\n\n\n\n\nPython (2.7, 3.5, 3.6)\n\n\nDocker\n installed and running\n\n\nConfigured \nawscli\n\n\n\n\nInstall sagify\n\n\nAt the command line:\n\n\npip install sagify\n\n\n\nGetting started\n\n\nStep 1: Clone Machine Learning demo repository\n\n\nYou're going to clone and train a Machine Learning codebase to train a classifer for the Iris data set.\n\n\nClone repository:\n\n\ngit clone https://github.com/Kenza-AI/sagify-demo.git\n\n\n\nOptionally, if you want to use Python 2.7 replace the value of \nREQUIRED_PYTHON\n and \nPYTHON_INTERPRETER\n in \ntest_environment.py\n and \nMakefile\n, respectively, to \npython2\n. \n\n\nCreate environment:\n\n\nmake create_environment\n\n\n\nDon't forget to activate the virtualenv after the creation of environment by executing \nworkon sagify-demo\n.\n\n\nInstall dependencies:\n\n\nmake requirements\n\n\n\nStep 2: Initialize sagify\n\n\nsagify init\n\n\n\nType in \nsagify-demo\n for SageMaker app name, \nN\n in question \nAre you starting a new project?\n, \nsrc\n for question \nType in the directory where your code lives\n and make sure to choose your preferred Python version, AWS profile and region. Finally, type \nrequirements.txt\n in question \nType in the path to requirements.txt\n.\n\n\nA module called \nsagify\n is created under the directory you provided. The structure is:\n\n\nsagify/\n    local_test/\n        test_dir/\n            input/\n                config/\n                    hyperparameters.json\n                data/\n                    training/\n            model/\n            output/\n        deploy_local.sh\n        train_local.sh\n    prediction/\n        __init__.py\n        nginx.conf\n        predict.py\n        prediction.py\n        predictor.py\n        serve\n        wsgi.py\n    training/\n        __init__.py\n        train\n        training.py\n    __init__.py\n    build.sh\n    Dockerfile\n    executor.sh\n    push.sh\n\n\n\nStep 3: Integrate sagify\n\n\nAs a Data Scientist, you only need to conduct a few actions. Sagify takes care of the rest:\n\n\n\n\nCopy a subset of training data under \nsagify/local_test/test_dir/input/data/training/\n to test that training works locally\n\n\nImplement \ntrain(...)\n function in \nsagify/training/training.py\n\n\nImplement \npredict(...)\n function in \nsagify/prediction/prediction.py\n\n\nOptionally, specify hyperparameters in \nsagify/local_test/test_dir/input/config/hyperparameters.json\n \n\n\n\n\nHence,\n\n\n\n\n\n\nCopy \niris.data\n files from \ndata\n to \nsagify/local_test/test_dir/input/data/training/\n\n\n\n\n\n\nReplace the \nTODOs\n in the \ntrain(...)\n function in \nsagify/training/training.py\n file with:\n\n\n    input_file_path = os.path.join(input_data_path, 'iris.data')\n    clf, accuracy = training_logic(input_file_path=input_file_path)\n\n    output_model_file_path = os.path.join(model_save_path, 'model.pkl')\n    joblib.dump(clf, output_model_file_path)\n\n    accuracy_report_file_path = os.path.join(model_save_path, 'report.txt')\n    with open(accuracy_report_file_path, 'w') as _out:\n        _out.write(str(accuracy))\n\n\n\nand at the top of the file, add:\n\n\nimport os\n\nfrom sklearn.externals import joblib\n\nfrom iris_training import train as training_logic\n\n\n\n\n\n\n\nReplace the body of \npredict(...)\n function in \nsagify/prediction/prediction.py\n with:\n\n\nmodel_input = json_input['features']\nprediction = ModelService.predict(model_input)\n\nreturn {\n    \"prediction\": prediction.item()\n}\n\n\n\nand replace the body of \nget_model()\n function in \nModelService\n class in the same file with:\n\n\nif cls.model is None:\n    from sklearn.externals import joblib\n    cls.model = joblib.load(os.path.join(_MODEL_PATH, 'model.pkl'))\nreturn cls.model\n\n\n\n\n\n\n\nStep 4: Build Docker image\n\n\nIt's time to build the Docker image that will contain the Machine Learning codebase:\n\n\nsagify build\n\n\n\nIf you run \ndocker images | grep sagify-demo\n in your terminal, you'll see the created Sagify-Demo image.\n\n\nStep 5: Train model\n\n\nTime to train the model for the Iris data set in the newly built Docker image:\n\n\nsagify local train\n\n\n\nModel file \nmodel.pkl\n and report file \nreport.txt\n are now under \nsagify/local_test/test_dir/model/\n\n\nStep 6: Deploy model\n\n\nFinally, serve the model as a REST Service:\n\n\nsagify local deploy\n\n\n\nRun the following curl command on your terminal to verify that the REST Service works:\n\n\ncurl -X POST \\\nhttp://localhost:8080/invocations \\\n-H 'Cache-Control: no-cache' \\\n-H 'Content-Type: application/json' \\\n-H 'Postman-Token: 41189b9a-40e2-abcf-b981-c31ae692072e' \\\n-d '{\n    \"features\":[[0.34, 0.45, 0.45, 0.3]]\n}'\n\n\n\nIt will be slow in the first couple of calls as it loads the model in a lazy manner.\n\n\nVoila! That's a proof that this Machine Learning model is going to be trained and deployed on AWS SageMaker successfully. Now, go to the \nUsage\n section in \nSagify Docs\n to see how to train and deploy this Machine Learning model to AWS SageMaker!\n\n\nUsage\n\n\nConfigure AWS Account\n\n\n\n\nSign in to the AWS Management Console as an IAM user and open the IAM console at \nhttps://console.aws.amazon.com/iam/\n\n\nSelect \nRoles\n from the list in the left-hand side, and click on \nCreate role\n\n\nThen, select \nSageMaker\n as the image shows:\n\n\n\n\n\n\n\n\nClick \nNext: Review\n on the following page:\n\n\n\n\n\n\n\n\nType a name for the SageMaker role, and click on \nCreate role\n:\n\n\n\n\n\n\n\n\nClick on the created role:\n\n\n\n\n\n\n\n\nClick on \nAttach policy\n and search for \nAmazonEC2ContainerRegistryFullAccess\n. Attach the corresponding policy:\n\n\n\n\n\n\n\n\nDo the same to attach the \nAmazonS3FullAccess\n and \nIAMReadOnlyAccess\n policies, and end up with the following:\n\n\n\n\n\n\n\n\n\n\nNow, go to Users page by clicking on \nUsers\n on the left-hand side.\n\n\n\n\n\n\nClick on your IAM user that you want to use for AWS SageMaker:\n\n\n\n\n\n\n\n\n\n\nCopy the ARN of that user:\n\n\n\n\n\n\n\n\nThen, go back the page of the Role you created and click on the \nTrust relationships\n tab:\n\n\n\n\n\n\n\n\n\n\nClick on \nEdit trust relationship\n and add the following:\n\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"PASTE_THE_ARN_YOU_COPIED_EARLIER\",\n                \"Service\": \"sagemaker.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n\n\n\n\n\n\n\nYou're almost there! Make sure that you have added the IAM user in your \n~/.aws/credentials\n file. For example:\n\n\n[test-sagemaker]\naws_access_key_id = ...\naws_secret_access_key = ...\n\n\n\n\n\n\n\nAnd, finally, add the following in the \n~/.aws/config\n file:\n\n\n[profile test-sagemaker]\nregion = us-east-1 <-- USE YOUR PREFERRED REGION\nrole_arn = COPY_PASTE_THE_ARN_OF_THE_CREATED_ROLE_NOT_USER! for example: arn:aws:iam::...:role/TestSageMakerRole\nsource_profile = test-sagemaker\n\n\n\n\n\n\n\nThat's it! From now on, choose the created AWS profile when initializing sagify.\n\n\n\n\n\n\nYou can change the AWS profile in an already initialized sagify module by changing the value of \naws_profile\n and \nprofile\n in \nsagify/config.json\n and \nsagify/push.sh\n, respectively.\n\n\n\n\n\n\nPush Docker Image to AWS ECS\n\n\nIf you have followed all the steps of \nGetting Started\n, run \nsagify push src\n to push the Docker image to AWS ECS. This step may take some time depending on your internet connection upload speed.\n\n\nCreate S3 Bucket\n\n\nMake sure to create an S3 bucket with a name of your choice, for example: \nsagify-demo\n\n\nUpload Training Data\n\n\nExecute \nsagify cloud upload-data -i data/ -s s3://sagify-demo/training-data\n to upload training data to S3\n\n\nTrain on AWS SageMaker\n\n\nExecute \nsagify cloud train -i s3://sagify-demo/training-data/ -o s3://sagify-demo/output/ -e ml.m4.xlarge\n to train the Machine Learning model on SageMaker. This command will use the pushed Docker image.\n\n\nCopy the displayed Model S3 location after the command is executed (example: \ns3://sagify-demo/output/sagify-demo-2018-04-29-15-04-14-483/output/model.tar.gz\n)\n\n\nDeploy on AWS SageMaker\n\n\nExecute \nsagify cloud deploy -m s3://sagify-demo/output/.../output/model.tar.gz -n 3 -e ml.m4.xlarge\n to deploy the model on SageMaker.\n\n\nCall SageMaker REST Endpoint\n\n\nFind the endpoint URL under \nEndpoints\n in AWS SageMaker service on AWS console. Please, refer to \nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-use-postman-to-call-api.html\n on how to call it from Postman as authorization is required.\n\n\nRemember that it's a POST HTTP request with Content-Type \napplication/json\n, and the request JSON body is of the form:\n\n\n    {\n        \"features\":[[0.34, 0.45, 0.45, 0.3]]\n    }\n\n\n\nHyperparameter Optimization\n\n\nGiven that you have configured your AWS Account as described in the previous section, you're now ready to perform Bayesian Hyperparameter Optimization on AWS SageMaker! The process is similar to training step.\n\n\nStep 1: Define Hyperparameter Configuration File\n\n\nDefine the Hyperparameter Configuration File. More specifically, you need to specify in a local JSON file the ranges for the hyperparameters, the name of the objective metric and its type (i.e. \nMaximize\n or \nMinimize\n). For example:\n\n\n{\n    \"ParameterRanges\": {\n        \"CategoricalParameterRanges\": [\n            {\n                \"Name\": \"kernel\",\n                \"Values\": [\"linear\", \"rbf\"]\n            }\n        ],\n        \"ContinuousParameterRanges\": [\n        {\n          \"MinValue\": 0.001,\n          \"MaxValue\": 10,\n          \"Name\": \"gamma\"\n        }\n        ],\n        \"IntegerParameterRanges\": [\n            {\n                \"Name\": \"C\",\n                \"MinValue\": 1,\n                \"MaxValue\": 10\n            }\n        ]\n    },\n    \"ObjectiveMetric\": {\n        \"Name\": \"Precision\",\n        \"Type\": \"Maximize\"\n    }\n}\n\n\n\n\nStep 2: Implement Train function\n\n\nReplace the \nTODOs\n in the \ntrain(...)\n function in \nsagify/training/training.py\n file with your logic. For example:\n\n\n    from sklearn import datasets\n    iris = datasets.load_iris()\n\n    # Read the hyperparameter config json file\n    import json\n    with open(hyperparams_path) as _in_file:\n        hyperparams_dict = json.load(_in_file)\n\n    from sklearn import svm\n    clf = svm.SVC(\n        gamma=float(hyperparams_dict['gamma']),  # Values will be read as strings, so make sure to convert them to the right data type\n        C=float(hyperparams_dict['C']),\n        kernel=hyperparams_dict['kernel']\n    )\n\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(\n        iris.data, iris.target, test_size=0.3, random_state=42)\n\n    clf.fit(X_train, y_train)\n\n    from sklearn.metrics import precision_score\n\n    predictions = clf.predict(X_test)\n\n    precision = precision_score(y_test, predictions, average='weighted')\n\n    # Log the objective metric name with its calculated value. In tis example is Precision.\n    # The objective name should be exactly the same with the one specified in the hyperparams congig json file.\n    # The value must be a numeric (float or int).\n    from sagify.api.hyperparameter_tuning import log_metric\n    name = \"Precision\"\n    log_metric(name, precision)\n\n    from joblib import dump\n    dump(clf, os.path.join(model_save_path, 'model.pkl'))\n\n    print('Training complete.')\n\n\n\nStep 3: Build and Push Docker image\n\n\n\n\nsagify build\n Make sure sagify is in your \nrequirements.txt\n file.\n\n\nsagify push\n\n\n\n\nStep 4: Call The CLI Command\n\n\nAnd, finally, call the hyperparameter-optimization CLI command. For example:\n\n\n sagify cloud hyperparameter_optimization -i s3://my-bucket/training-data/ -o s3://my-bucket/output/ -e ml.m4.xlarge -h local/path/to/hyperparam_ranges.json\n\n\n\nStep 5: Monitor Progress\n\n\nYou can monitor the progress via the SageMaker UI console. Here is an example of a finished Hyperparameter Optimization job:\n\n\n\n\nCommands\n\n\nInitialize\n\n\nName\n\n\nInitializes a sagify module\n\n\nSynopsis\n\n\nsagify init\n\n\n\nDescription\n\n\nThis command initializes a sagify module in the directory you provide when asked after you invoke the \ninit\n command.\n\n\nExample\n\n\nsagify init\n\n\n\nConfigure\n\n\nDescription\n\n\nUpdates an existing configuration value e.g. \npython version\n or \nAWS region\n.\n\n\nSynopsis\n\n\nsagify configure [--aws-region AWS_REGION] [--aws-profile AWS_PROFILE] [--image-name IMAGE_NAME] [--python-version PYTHON_VERSION]\n\n\n\nOptional Flags\n\n\n--aws-region AWS_REGION\n: \nAWS\n region where \nDocker\n images are pushed and \nSageMaker\n operations (\ntrain\n, \ndeploy\n) are performed.\n\n\n--aws-profile AWS_PROFILE\n: \nAWS\n profile to use when interacting with \nAWS\n.\n\n\n--image-name IMAGE_NAME\n: \nDocker\n image name used when building for use with \nSageMaker\n. This shows up as an \nAWS ECR\n repository on your \nAWS\n account.\n\n\n--python-version PYTHON_VERSION\n: \nPython\n version used when building \nSageMaker's\n \nDocker\n images. Curently supported versions: \n2.7\n , \n3.6\n.\n\n\nExample\n\n\nsagify configure --aws-region us-east-2 --aws-profile default --image-name sage-docker-image-name --python-version 3.6\n\n\n\nBuild\n\n\nName\n\n\nBuilds a Docker image\n\n\nSynopsis\n\n\nsagify build\n\n\n\nDescription\n\n\nThis command builds a Docker image from code under the directory sagify is installed in. A \nREQUIREMENTS_FILE\n needs to be specified during \nsagify init\n or later via \nsagify configure --requirements-dir\n for all required dependencies to be installed in the Docker image. \n\n\nExample\n\n\nsagify build\n\n\n\nLocal Train\n\n\nName\n\n\nExecutes a Docker image in train mode\n\n\nSynopsis\n\n\nsagify local train\n\n\n\nDescription\n\n\nThis command executes a Docker image in train mode. More specifically, it executes the \ntrain(...)\n function in \nsagify/training/training.py\n inside an already built Docker image (see Build command section).\n\n\nExample\n\n\nsagify local train\n\n\n\nLocal Deploy\n\n\nName\n\n\nExecutes a Docker image in serve mode\n\n\nSynopsis\n\n\nsagify local deploy\n\n\n\nDescription\n\n\nThis command executes a Docker image in serve mode. More specifically, it runs a Flask REST app in Docker image and directs HTTP requests to \n/invocations\n endpoint. Then, the \n/invocations\n endpoint calls the \npredict(...)\n function in \nsagify/prediction/prediction.py\n (see Build command section on how to build a Docker image).\n\n\nExample\n\n\nsagify local deploy\n\n\n\nPush\n\n\nName\n\n\nPushes a Docker image to AWS Elastic Container Service\n\n\nSynopsis\n\n\nsagify push [--aws-profile PROFILE_NAME] [--aws-region AWS_REGION] [--iam-role-arn IAM_ROLE] [--external-id EXTERNAL_ID]\n\n\n\nDescription\n\n\nThis command pushes an already built Docker image to AWS Elastic Container Service. Later on, AWS SageMaker will consume that image from AWS Elastic Container Service for train and serve mode.\n\n\n\n\nOnly one of \niam-role-arn\n and \naws_profile\n can be provided. \nexternal-id\n is ignored when no \niam-role-arn\n is provided.\n\n\n\n\nOptional Flags\n\n\n--iam-role-arn IAM_ROLE\n or \n-i IAM_ROLE\n: AWS IAM role to use for pushing to ECR\n\n\n--aws-region AWS_REGION\n or \n-r AWS_REGION\n: The AWS region to push the image to\n\n\n--aws-profile PROFILE_NAME\n or \n-p PROFILE_NAME\n: AWS profile to use for pushing to ECR\n\n\n--external-id EXTERNAL_ID\n or \n-e EXTERNAL_ID\n: Optional external id used when using an IAM role\n\n\nExample\n\n\nsagify push\n\n\n\nCloud Upload Data\n\n\nName\n\n\nUploads data to AWS S3\n\n\nSynopsis\n\n\nsagify cloud upload-data --input-dir LOCAL_INPUT_DATA_DIR --s3-dir S3_TARGET_DATA_LOCATION\n\n\n\nDescription\n\n\nThis command uploads content under \nLOCAL_INPUT_DATA_DIR\n to S3 under \nS3_TARGET_DATA_LOCATION\n\n\nRequired Flags\n\n\n--input-dir LOCAL_INPUT_DATA_DIR\n or \n-i LOCAL_INPUT_DATA_DIR\n: Local input directory\n\n\n--s3-dir S3_TARGET_DATA_LOCATION\n or \n-s S3_TARGET_DATA_LOCATION\n: S3 target location\n\n\nExample\n\n\nsagify cloud upload-data -i ./training_data/ -s s3://my-bucket/training-data/\n\n\n\nCloud Train\n\n\nName\n\n\nTrains your ML/DL model using a Docker image on AWS SageMaker with input from S3\n\n\nSynopsis\n\n\nsagify cloud train --input-s3-dir INPUT_DATA_S3_LOCATION --output-s3-dir S3_LOCATION_TO_SAVE_OUTPUT --ec2-type EC2_TYPE [--hyperparams-file HYPERPARAMS_JSON_FILE] [--volume-size EBS_SIZE_IN_GB] [--time-out TIME_OUT_IN_SECS] [--aws-tags TAGS] [--iam-role-arn IAM_ROLE] [--external-id EXTERNAL_ID] [--base-job-name BASE_JOB_NAME] [--job-name JOB_NAME] [--metric-names COMMA_SEPARATED_METRIC_NAMES]\n\n\n\nDescription\n\n\nThis command retrieves a Docker image from AWS Elastic Container Service and executes it on AWS SageMaker in train mode\n\n\nRequired Flags\n\n\n--input-s3-dir INPUT_DATA_S3_LOCATION\n or \n-i INPUT_DATA_S3_LOCATION\n: S3 location to input data\n\n\n--output-s3-dir S3_LOCATION_TO_SAVE_OUTPUT\n or \n-o S3_LOCATION_TO_SAVE_OUTPUT\n: S3 location to save output (models, reports, etc). Make sure that the output bucket already exists. Any not existing key prefix will be created by sagify.\n\n\n--ec2-type EC2_TYPE\n or \n-e EC2_TYPE\n: ec2 type. Refer to \nhttps://aws.amazon.com/sagemaker/pricing/instance-types/\n\n\nOptional Flags\n\n\n--hyperparams-file HYPERPARAMS_JSON_FILE\n or \n-h HYPERPARAMS_JSON_FILE\n: Path to hyperparams JSON file\n\n\n--volume-size EBS_SIZE_IN_GB\n or \n-v EBS_SIZE_IN_GB\n: Size in GB of the EBS volume (default: 30)\n\n\n--time-out TIME_OUT_IN_SECS\n or \n-s TIME_OUT_IN_SECS\n: Time-out in seconds (default: 24 * 60 * 60)\n\n\n--aws-tags TAGS\n or \n-a TAGS\n: Tags for labeling a training job of the form \ntag1=value1;tag2=value2\n. For more, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n\n\n--iam-role-arn IAM_ROLE\n or \n-r IAM_ROLE\n: AWS IAM role to use for training with \nSageMaker\n\n\n--external-id EXTERNAL_ID\n or \n-x EXTERNAL_ID\n: Optional external id used when using an IAM role\n\n\n--base-job-name BASE_JOB_NAME\n or \n-n BASE_JOB_NAME\n: Optional prefix for the SageMaker training job\n\n\n--job-name JOB_NAME\n: Optional name for the SageMaker training job. NOTE: if a \n--base-job-name\n is passed along with this option, it will be ignored.\n\n\n--metric-names COMMA_SEPARATED_METRIC_NAMES\n: Optional comma-separated metric names for tracking performance of training jobs. Example: \nPrecision,Recall,AUC\n. Then, make sure you log these metric values using the \nlog_metric\n function in the \ntrain\n function:\n\n\n```\n...\nfrom sagify.api.hyperparameter_tuning import log_metric\nlog_metric(\"Precision:, precision)\nlog_metric(\"Accuracy\", accuracy)\n...\n```\n\n\n\nWhen the training jobs finishes, they will be stored in the CloudWatch algorithm metrics logs of the SageMaker training job:\n\n\n\n\nExample\n\n\nsagify cloud train -i s3://my-bucket/training-data/ -o s3://my-bucket/output/ -e ml.m4.xlarge -h local/path/to/hyperparams.json -v 60 -t 86400 --metric-names Accuracy,Precision\n\n\n\nCloud Hyperparameter Optimization\n\n\nName\n\n\nExecutes a Docker image in hyperparameter-optimization mode on AWS SageMaker\n\n\nSynopsis\n\n\nsagify cloud hyperparameter_optimization --input-s3-dir INPUT_DATA_S3_LOCATION --output-s3-dir S3_LOCATION_TO_SAVE_MULTIPLE_TRAINED_MODELS --ec2-type EC2_TYPE [--hyperparams-config-file HYPERPARAM_RANGES_JSON_FILE] [--max-jobs MAX_NUMBER_OF_TRAINING_JOBS] [--max-parallel-jobs MAX_NUMBER_OF_PARALLEL_TRAINING_JOBS] [--volume-size EBS_SIZE_IN_GB] [--time-out TIME_OUT_IN_SECS] [--aws-tags TAGS] [--iam-role-arn IAM_ROLE] [--external-id EXTERNAL_ID] [--base-job-name BASE_JOB_NAME] [--job-name JOB_NAME] [--wait WAIT_UNTIL_HYPERPARAM_JOB_IS_FINISHED]\n\n\n\nDescription\n\n\nThis command retrieves a Docker image from AWS Elastic Container Service and executes it on AWS SageMaker in hyperparameter-optimization mode\n\n\nRequired Flags\n\n\n--input-s3-dir INPUT_DATA_S3_LOCATION\n or \n-i INPUT_DATA_S3_LOCATION\n: S3 location to input data\n\n\n--output-s3-dir S3_LOCATION_TO_SAVE_OUTPUT\n or \n-o S3_LOCATION_TO_SAVE_OUTPUT\n: S3 location to save output (models, reports, etc). Make sure that the output bucket already exists. Any not existing key prefix will be created by sagify.\n\n\n--ec2-type EC2_TYPE\n or \n-e EC2_TYPE\n: ec2 type. Refer to \nhttps://aws.amazon.com/sagemaker/pricing/instance-types/\n\n\n--hyperparams-config-file HYPERPARAM_RANGES_JSON_FILE\n or \n-h HYPERPARAM_RANGES_JSON_FILE\n: Local path to hyperparameters configuration file. Example:\n\n\n{\n    \"ParameterRanges\": {\n        \"CategoricalParameterRanges\": [\n            {\n                \"Name\": \"kernel\",\n                \"Values\": [\"linear\", \"rbf\"]\n            }\n        ],\n        \"ContinuousParameterRanges\": [\n        {\n          \"MinValue\": 0.001,\n          \"MaxValue\": 10,\n          \"Name\": \"gamma\"\n        }\n        ],\n        \"IntegerParameterRanges\": [\n            {\n                \"Name\": \"C\",\n                \"MinValue\": 1,\n                \"MaxValue\": 10\n            }\n        ]\n    },\n    \"ObjectiveMetric\": {\n        \"Name\": \"Precision\",\n        \"Type\": \"Maximize\"\n    }\n}\n\n\n\n\nOptional Flags\n\n\n--max-jobs MAX_NUMBER_OF_TRAINING_JOBS\n or \n-m MAX_NUMBER_OF_TRAINING_JOBS\n: Maximum total number of training jobs to start for the hyperparameter tuning job (default: 3)\n\n\n--max-parallel-jobs MAX_NUMBER_OF_PARALLEL_TRAINING_JOBS\n or \n-p MAX_NUMBER_OF_PARALLEL_TRAINING_JOBS\n: Maximum number of parallel training jobs to start (default: 1)\n\n\n--volume-size EBS_SIZE_IN_GB\n or \n-v EBS_SIZE_IN_GB\n: Size in GB of the EBS volume (default: 30)\n\n\n--time-out TIME_OUT_IN_SECS\n or \n-s TIME_OUT_IN_SECS\n: Time-out in seconds (default: 24 * 60 * 60)\n\n\n--aws-tags TAGS\n or \n-a TAGS\n: Tags for labeling a training job of the form \ntag1=value1;tag2=value2\n. For more, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n\n\n--iam-role-arn IAM_ROLE\n or \n-r IAM_ROLE\n: AWS IAM role to use for training with \nSageMaker\n\n\n--external-id EXTERNAL_ID\n or \n-x EXTERNAL_ID\n: Optional external id used when using an IAM role\n\n\n--base-job-name BASE_JOB_NAME\n or \n-n BASE_JOB_NAME\n: Optional prefix for the SageMaker training job\n\n\n--job-name JOB_NAME\n: Optional name for the SageMaker training job. NOTE: if a \n--base-job-name\n is passed along with this option, it will be ignored. \n\n\n--wait WAIT_UNTIL_HYPERPARAM_JOB_IS_FINISHED\n or \n-w WAIT_UNTIL_HYPERPARAM_JOB_IS_FINISHED\n: Optional flag to wait until Hyperparameter Tuning is finished. (default: don't wait) \n\n\nExample\n\n\nsagify cloud hyperparameter_optimization -i s3://my-bucket/training-data/ -o s3://my-bucket/output/ -e ml.m4.xlarge -h local/path/to/hyperparam_ranges.json -v 60 -t 86400\n\n\n\nCloud Deploy\n\n\nName\n\n\nExecutes a Docker image in serve mode on AWS SageMaker\n\n\nSynopsis\n\n\nsagify cloud deploy --s3-model-location S3_LOCATION_TO_MODEL_TAR_GZ --num-instance NUMBER_OF_EC2_INSTANCES --ec2-type EC2_TYPE [--aws-tags TAGS] [--iam-role-arn IAM_ROLE] [--external-id EXTERNAL_ID] [--endpoint-name ENDPOINT_NAME]\n\n\n\nDescription\n\n\nThis command retrieves a Docker image from AWS Elastic Container Service and executes it on AWS SageMaker in serve mode. You can update an endpoint (model or number of instances) by specifying the endpoint-name.\n\n\nRequired Flags\n\n\n--s3-model-location S3_LOCATION_TO_MODEL_TAR_GZ\n or \n-m S3_LOCATION_TO_MODEL_TAR_GZ\n: S3 location to to model tar.gz\n\n\n--num-instances NUMBER_OF_EC2_INSTANCES\n or \nn NUMBER_OF_EC2_INSTANCES\n: Number of ec2 instances\n\n\n--ec2-type EC2_TYPE\n or \ne EC2_TYPE\n: ec2 type. Refer to https://aws.amazon.com/sagemaker/pricing/instance-types/\n\n\nOptional Flags\n\n\n--aws-tags TAGS\n or \n-a TAGS\n: Tags for labeling a training job of the form \ntag1=value1;tag2=value2\n. For more, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n\n\n--iam-role-arn IAM_ROLE\n or \n-r IAM_ROLE\n: AWS IAM role to use for deploying with \nSageMaker\n\n\n--external-id EXTERNAL_ID\n or \n-x EXTERNAL_ID\n: Optional external id used when using an IAM role\n\n\n--endpoint-name ENDPOINT_NAME\n: Optional name for the SageMaker endpoint\n\n\nExample\n\n\nsagify cloud deploy -m s3://my-bucket/output/model.tar.gz -n 3 -e ml.m4.xlarge\n\n\n\nCloud Batch Transform\n\n\nName\n\n\nExecutes a Docker image in batch transform mode on AWS SageMaker, i.e. runs batch predictions on user defined S3 data\n\n\nSynopsis\n\n\nsagify cloud batch_transform --s3-model-location S3_LOCATION_TO_MODEL_TAR_GZ --s3-input-location S3_INPUT_LOCATION --s3-output-location S3_OUTPUT_LOCATION --num-instance NUMBER_OF_EC2_INSTANCES --ec2-type EC2_TYPE [--aws-tags TAGS] [--iam-role-arn IAM_ROLE] [--external-id EXTERNAL_ID] [--wait WAIT_UNTIL_BATCH_TRANSFORM_JOB_IS_FINISHED] [--job-name JOB_NAME]\n\n\n\nDescription\n\n\nThis command retrieves a Docker image from AWS Elastic Container Service and executes it on AWS SageMaker in batch transform mode, i.e. runs batch predictions on user defined S3 data. SageMaker will spin up REST container(s) and call it/them with input data(features) from a user defined S3 path.\n\n\nThings to do:\n- You should implement the predict function that expects a JSON containing the required feature values. It's the same predict function used for deploying the model as a REST service. Example of a JSON:\n\n\n{\n    \"features\": [5.1,3.5,1.4,0.2]\n}\n\n\n\n\n\n\nThe input S3 path should contain a file or multiple files where each line is a JSON, the same JSON format as the one expected in the predict function. Example of a file:\n\n\n\n\n{\"features\": [5.1,3.5,1.4,0.2]}\n{\"features\": [4.9,3.0,1.4,0.2]}\n{\"features\": [4.7,3.2,1.3,0.2]}\n{\"features\": [4.6,3.1,1.5,0.2]}\n\n\n\n\nRequired Flags\n\n\n--s3-model-location S3_LOCATION_TO_MODEL_TAR_GZ\n or \n-m S3_LOCATION_TO_MODEL_TAR_GZ\n: S3 location to to model tar.gz\n\n\n--s3-input-location S3_INPUT_LOCATION\n or \n-i S3_INPUT_LOCATION\n: s3 input data location\n\n\n--s3-output-location S3_OUTPUT_LOCATION\n or \n-o S3_OUTPUT_LOCATION\n: s3 location to save predictions\n\n\n--num-instances NUMBER_OF_EC2_INSTANCES\n or \nn NUMBER_OF_EC2_INSTANCES\n: Number of ec2 instances\n\n\n--ec2-type EC2_TYPE\n or \ne EC2_TYPE\n: ec2 type. Refer to https://aws.amazon.com/sagemaker/pricing/instance-types/\n\n\nOptional Flags\n\n\n--aws-tags TAGS\n or \n-a TAGS\n: Tags for labeling a training job of the form \ntag1=value1;tag2=value2\n. For more, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n\n\n--iam-role-arn IAM_ROLE\n or \n-r IAM_ROLE\n: AWS IAM role to use for deploying with \nSageMaker\n\n\n--external-id EXTERNAL_ID\n or \n-x EXTERNAL_ID\n: Optional external id used when using an IAM role\n\n\n--wait WAIT_UNTIL_BATCH_TRANSFORM_JOB_IS_FINISHED\n or \n-w WAIT_UNTIL_BATCH_TRANSFORM_JOB_IS_FINISHED\n: Optional flag to wait until Batch Transform is finished. (default: don't wait)\n\n\n--job-name JOB_NAME\n: Optional name for the SageMaker batch transform job\n\n\nExample\n\n\nsagify cloud batch_transform -m s3://my-bucket/output/model.tar.gz -i s3://my-bucket/input_features -o s3://my-bucket/predictions -n 3 -e ml.m4.xlarge",
            "title": "sagify"
        },
        {
            "location": "/#sagify",
            "text": "A command-line utility to train and deploy Machine Learning/Deep Learning models on  AWS SageMaker  in a few simple steps!",
            "title": "sagify"
        },
        {
            "location": "/#installation",
            "text": "",
            "title": "Installation"
        },
        {
            "location": "/#prerequisites",
            "text": "sagify requires the following:   Python (2.7, 3.5, 3.6)  Docker  installed and running  Configured  awscli",
            "title": "Prerequisites"
        },
        {
            "location": "/#install-sagify",
            "text": "At the command line:  pip install sagify",
            "title": "Install sagify"
        },
        {
            "location": "/#getting-started",
            "text": "",
            "title": "Getting started"
        },
        {
            "location": "/#step-1-clone-machine-learning-demo-repository",
            "text": "You're going to clone and train a Machine Learning codebase to train a classifer for the Iris data set.  Clone repository:  git clone https://github.com/Kenza-AI/sagify-demo.git  Optionally, if you want to use Python 2.7 replace the value of  REQUIRED_PYTHON  and  PYTHON_INTERPRETER  in  test_environment.py  and  Makefile , respectively, to  python2 .   Create environment:  make create_environment  Don't forget to activate the virtualenv after the creation of environment by executing  workon sagify-demo .  Install dependencies:  make requirements",
            "title": "Step 1: Clone Machine Learning demo repository"
        },
        {
            "location": "/#step-2-initialize-sagify",
            "text": "sagify init  Type in  sagify-demo  for SageMaker app name,  N  in question  Are you starting a new project? ,  src  for question  Type in the directory where your code lives  and make sure to choose your preferred Python version, AWS profile and region. Finally, type  requirements.txt  in question  Type in the path to requirements.txt .  A module called  sagify  is created under the directory you provided. The structure is:  sagify/\n    local_test/\n        test_dir/\n            input/\n                config/\n                    hyperparameters.json\n                data/\n                    training/\n            model/\n            output/\n        deploy_local.sh\n        train_local.sh\n    prediction/\n        __init__.py\n        nginx.conf\n        predict.py\n        prediction.py\n        predictor.py\n        serve\n        wsgi.py\n    training/\n        __init__.py\n        train\n        training.py\n    __init__.py\n    build.sh\n    Dockerfile\n    executor.sh\n    push.sh",
            "title": "Step 2: Initialize sagify"
        },
        {
            "location": "/#step-3-integrate-sagify",
            "text": "As a Data Scientist, you only need to conduct a few actions. Sagify takes care of the rest:   Copy a subset of training data under  sagify/local_test/test_dir/input/data/training/  to test that training works locally  Implement  train(...)  function in  sagify/training/training.py  Implement  predict(...)  function in  sagify/prediction/prediction.py  Optionally, specify hyperparameters in  sagify/local_test/test_dir/input/config/hyperparameters.json     Hence,    Copy  iris.data  files from  data  to  sagify/local_test/test_dir/input/data/training/    Replace the  TODOs  in the  train(...)  function in  sagify/training/training.py  file with:      input_file_path = os.path.join(input_data_path, 'iris.data')\n    clf, accuracy = training_logic(input_file_path=input_file_path)\n\n    output_model_file_path = os.path.join(model_save_path, 'model.pkl')\n    joblib.dump(clf, output_model_file_path)\n\n    accuracy_report_file_path = os.path.join(model_save_path, 'report.txt')\n    with open(accuracy_report_file_path, 'w') as _out:\n        _out.write(str(accuracy))  and at the top of the file, add:  import os\n\nfrom sklearn.externals import joblib\n\nfrom iris_training import train as training_logic    Replace the body of  predict(...)  function in  sagify/prediction/prediction.py  with:  model_input = json_input['features']\nprediction = ModelService.predict(model_input)\n\nreturn {\n    \"prediction\": prediction.item()\n}  and replace the body of  get_model()  function in  ModelService  class in the same file with:  if cls.model is None:\n    from sklearn.externals import joblib\n    cls.model = joblib.load(os.path.join(_MODEL_PATH, 'model.pkl'))\nreturn cls.model",
            "title": "Step 3: Integrate sagify"
        },
        {
            "location": "/#step-4-build-docker-image",
            "text": "It's time to build the Docker image that will contain the Machine Learning codebase:  sagify build  If you run  docker images | grep sagify-demo  in your terminal, you'll see the created Sagify-Demo image.",
            "title": "Step 4: Build Docker image"
        },
        {
            "location": "/#step-5-train-model",
            "text": "Time to train the model for the Iris data set in the newly built Docker image:  sagify local train  Model file  model.pkl  and report file  report.txt  are now under  sagify/local_test/test_dir/model/",
            "title": "Step 5: Train model"
        },
        {
            "location": "/#step-6-deploy-model",
            "text": "Finally, serve the model as a REST Service:  sagify local deploy  Run the following curl command on your terminal to verify that the REST Service works:  curl -X POST \\\nhttp://localhost:8080/invocations \\\n-H 'Cache-Control: no-cache' \\\n-H 'Content-Type: application/json' \\\n-H 'Postman-Token: 41189b9a-40e2-abcf-b981-c31ae692072e' \\\n-d '{\n    \"features\":[[0.34, 0.45, 0.45, 0.3]]\n}'  It will be slow in the first couple of calls as it loads the model in a lazy manner.  Voila! That's a proof that this Machine Learning model is going to be trained and deployed on AWS SageMaker successfully. Now, go to the  Usage  section in  Sagify Docs  to see how to train and deploy this Machine Learning model to AWS SageMaker!",
            "title": "Step 6: Deploy model"
        },
        {
            "location": "/#usage",
            "text": "",
            "title": "Usage"
        },
        {
            "location": "/#configure-aws-account",
            "text": "Sign in to the AWS Management Console as an IAM user and open the IAM console at  https://console.aws.amazon.com/iam/  Select  Roles  from the list in the left-hand side, and click on  Create role  Then, select  SageMaker  as the image shows:     Click  Next: Review  on the following page:     Type a name for the SageMaker role, and click on  Create role :     Click on the created role:     Click on  Attach policy  and search for  AmazonEC2ContainerRegistryFullAccess . Attach the corresponding policy:     Do the same to attach the  AmazonS3FullAccess  and  IAMReadOnlyAccess  policies, and end up with the following:      Now, go to Users page by clicking on  Users  on the left-hand side.    Click on your IAM user that you want to use for AWS SageMaker:      Copy the ARN of that user:     Then, go back the page of the Role you created and click on the  Trust relationships  tab:      Click on  Edit trust relationship  and add the following:  {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"PASTE_THE_ARN_YOU_COPIED_EARLIER\",\n                \"Service\": \"sagemaker.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}    You're almost there! Make sure that you have added the IAM user in your  ~/.aws/credentials  file. For example:  [test-sagemaker]\naws_access_key_id = ...\naws_secret_access_key = ...    And, finally, add the following in the  ~/.aws/config  file:  [profile test-sagemaker]\nregion = us-east-1 <-- USE YOUR PREFERRED REGION\nrole_arn = COPY_PASTE_THE_ARN_OF_THE_CREATED_ROLE_NOT_USER! for example: arn:aws:iam::...:role/TestSageMakerRole\nsource_profile = test-sagemaker    That's it! From now on, choose the created AWS profile when initializing sagify.    You can change the AWS profile in an already initialized sagify module by changing the value of  aws_profile  and  profile  in  sagify/config.json  and  sagify/push.sh , respectively.",
            "title": "Configure AWS Account"
        },
        {
            "location": "/#push-docker-image-to-aws-ecs",
            "text": "If you have followed all the steps of  Getting Started , run  sagify push src  to push the Docker image to AWS ECS. This step may take some time depending on your internet connection upload speed.",
            "title": "Push Docker Image to AWS ECS"
        },
        {
            "location": "/#create-s3-bucket",
            "text": "Make sure to create an S3 bucket with a name of your choice, for example:  sagify-demo",
            "title": "Create S3 Bucket"
        },
        {
            "location": "/#upload-training-data",
            "text": "Execute  sagify cloud upload-data -i data/ -s s3://sagify-demo/training-data  to upload training data to S3",
            "title": "Upload Training Data"
        },
        {
            "location": "/#train-on-aws-sagemaker",
            "text": "Execute  sagify cloud train -i s3://sagify-demo/training-data/ -o s3://sagify-demo/output/ -e ml.m4.xlarge  to train the Machine Learning model on SageMaker. This command will use the pushed Docker image.  Copy the displayed Model S3 location after the command is executed (example:  s3://sagify-demo/output/sagify-demo-2018-04-29-15-04-14-483/output/model.tar.gz )",
            "title": "Train on AWS SageMaker"
        },
        {
            "location": "/#deploy-on-aws-sagemaker",
            "text": "Execute  sagify cloud deploy -m s3://sagify-demo/output/.../output/model.tar.gz -n 3 -e ml.m4.xlarge  to deploy the model on SageMaker.",
            "title": "Deploy on AWS SageMaker"
        },
        {
            "location": "/#call-sagemaker-rest-endpoint",
            "text": "Find the endpoint URL under  Endpoints  in AWS SageMaker service on AWS console. Please, refer to  https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-use-postman-to-call-api.html  on how to call it from Postman as authorization is required.  Remember that it's a POST HTTP request with Content-Type  application/json , and the request JSON body is of the form:      {\n        \"features\":[[0.34, 0.45, 0.45, 0.3]]\n    }",
            "title": "Call SageMaker REST Endpoint"
        },
        {
            "location": "/#hyperparameter-optimization",
            "text": "Given that you have configured your AWS Account as described in the previous section, you're now ready to perform Bayesian Hyperparameter Optimization on AWS SageMaker! The process is similar to training step.",
            "title": "Hyperparameter Optimization"
        },
        {
            "location": "/#step-1-define-hyperparameter-configuration-file",
            "text": "Define the Hyperparameter Configuration File. More specifically, you need to specify in a local JSON file the ranges for the hyperparameters, the name of the objective metric and its type (i.e.  Maximize  or  Minimize ). For example:  {\n    \"ParameterRanges\": {\n        \"CategoricalParameterRanges\": [\n            {\n                \"Name\": \"kernel\",\n                \"Values\": [\"linear\", \"rbf\"]\n            }\n        ],\n        \"ContinuousParameterRanges\": [\n        {\n          \"MinValue\": 0.001,\n          \"MaxValue\": 10,\n          \"Name\": \"gamma\"\n        }\n        ],\n        \"IntegerParameterRanges\": [\n            {\n                \"Name\": \"C\",\n                \"MinValue\": 1,\n                \"MaxValue\": 10\n            }\n        ]\n    },\n    \"ObjectiveMetric\": {\n        \"Name\": \"Precision\",\n        \"Type\": \"Maximize\"\n    }\n}",
            "title": "Step 1: Define Hyperparameter Configuration File"
        },
        {
            "location": "/#step-2-implement-train-function",
            "text": "Replace the  TODOs  in the  train(...)  function in  sagify/training/training.py  file with your logic. For example:      from sklearn import datasets\n    iris = datasets.load_iris()\n\n    # Read the hyperparameter config json file\n    import json\n    with open(hyperparams_path) as _in_file:\n        hyperparams_dict = json.load(_in_file)\n\n    from sklearn import svm\n    clf = svm.SVC(\n        gamma=float(hyperparams_dict['gamma']),  # Values will be read as strings, so make sure to convert them to the right data type\n        C=float(hyperparams_dict['C']),\n        kernel=hyperparams_dict['kernel']\n    )\n\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(\n        iris.data, iris.target, test_size=0.3, random_state=42)\n\n    clf.fit(X_train, y_train)\n\n    from sklearn.metrics import precision_score\n\n    predictions = clf.predict(X_test)\n\n    precision = precision_score(y_test, predictions, average='weighted')\n\n    # Log the objective metric name with its calculated value. In tis example is Precision.\n    # The objective name should be exactly the same with the one specified in the hyperparams congig json file.\n    # The value must be a numeric (float or int).\n    from sagify.api.hyperparameter_tuning import log_metric\n    name = \"Precision\"\n    log_metric(name, precision)\n\n    from joblib import dump\n    dump(clf, os.path.join(model_save_path, 'model.pkl'))\n\n    print('Training complete.')",
            "title": "Step 2: Implement Train function"
        },
        {
            "location": "/#step-3-build-and-push-docker-image",
            "text": "sagify build  Make sure sagify is in your  requirements.txt  file.  sagify push",
            "title": "Step 3: Build and Push Docker image"
        },
        {
            "location": "/#step-4-call-the-cli-command",
            "text": "And, finally, call the hyperparameter-optimization CLI command. For example:   sagify cloud hyperparameter_optimization -i s3://my-bucket/training-data/ -o s3://my-bucket/output/ -e ml.m4.xlarge -h local/path/to/hyperparam_ranges.json",
            "title": "Step 4: Call The CLI Command"
        },
        {
            "location": "/#step-5-monitor-progress",
            "text": "You can monitor the progress via the SageMaker UI console. Here is an example of a finished Hyperparameter Optimization job:",
            "title": "Step 5: Monitor Progress"
        },
        {
            "location": "/#commands",
            "text": "",
            "title": "Commands"
        },
        {
            "location": "/#initialize",
            "text": "",
            "title": "Initialize"
        },
        {
            "location": "/#name",
            "text": "Initializes a sagify module",
            "title": "Name"
        },
        {
            "location": "/#synopsis",
            "text": "sagify init",
            "title": "Synopsis"
        },
        {
            "location": "/#description",
            "text": "This command initializes a sagify module in the directory you provide when asked after you invoke the  init  command.",
            "title": "Description"
        },
        {
            "location": "/#example",
            "text": "sagify init",
            "title": "Example"
        },
        {
            "location": "/#configure",
            "text": "",
            "title": "Configure"
        },
        {
            "location": "/#description_1",
            "text": "Updates an existing configuration value e.g.  python version  or  AWS region .",
            "title": "Description"
        },
        {
            "location": "/#synopsis_1",
            "text": "sagify configure [--aws-region AWS_REGION] [--aws-profile AWS_PROFILE] [--image-name IMAGE_NAME] [--python-version PYTHON_VERSION]",
            "title": "Synopsis"
        },
        {
            "location": "/#optional-flags",
            "text": "--aws-region AWS_REGION :  AWS  region where  Docker  images are pushed and  SageMaker  operations ( train ,  deploy ) are performed.  --aws-profile AWS_PROFILE :  AWS  profile to use when interacting with  AWS .  --image-name IMAGE_NAME :  Docker  image name used when building for use with  SageMaker . This shows up as an  AWS ECR  repository on your  AWS  account.  --python-version PYTHON_VERSION :  Python  version used when building  SageMaker's   Docker  images. Curently supported versions:  2.7  ,  3.6 .",
            "title": "Optional Flags"
        },
        {
            "location": "/#example_1",
            "text": "sagify configure --aws-region us-east-2 --aws-profile default --image-name sage-docker-image-name --python-version 3.6",
            "title": "Example"
        },
        {
            "location": "/#build",
            "text": "",
            "title": "Build"
        },
        {
            "location": "/#name_1",
            "text": "Builds a Docker image",
            "title": "Name"
        },
        {
            "location": "/#synopsis_2",
            "text": "sagify build",
            "title": "Synopsis"
        },
        {
            "location": "/#description_2",
            "text": "This command builds a Docker image from code under the directory sagify is installed in. A  REQUIREMENTS_FILE  needs to be specified during  sagify init  or later via  sagify configure --requirements-dir  for all required dependencies to be installed in the Docker image.",
            "title": "Description"
        },
        {
            "location": "/#example_2",
            "text": "sagify build",
            "title": "Example"
        },
        {
            "location": "/#local-train",
            "text": "",
            "title": "Local Train"
        },
        {
            "location": "/#name_2",
            "text": "Executes a Docker image in train mode",
            "title": "Name"
        },
        {
            "location": "/#synopsis_3",
            "text": "sagify local train",
            "title": "Synopsis"
        },
        {
            "location": "/#description_3",
            "text": "This command executes a Docker image in train mode. More specifically, it executes the  train(...)  function in  sagify/training/training.py  inside an already built Docker image (see Build command section).",
            "title": "Description"
        },
        {
            "location": "/#example_3",
            "text": "sagify local train",
            "title": "Example"
        },
        {
            "location": "/#local-deploy",
            "text": "",
            "title": "Local Deploy"
        },
        {
            "location": "/#name_3",
            "text": "Executes a Docker image in serve mode",
            "title": "Name"
        },
        {
            "location": "/#synopsis_4",
            "text": "sagify local deploy",
            "title": "Synopsis"
        },
        {
            "location": "/#description_4",
            "text": "This command executes a Docker image in serve mode. More specifically, it runs a Flask REST app in Docker image and directs HTTP requests to  /invocations  endpoint. Then, the  /invocations  endpoint calls the  predict(...)  function in  sagify/prediction/prediction.py  (see Build command section on how to build a Docker image).",
            "title": "Description"
        },
        {
            "location": "/#example_4",
            "text": "sagify local deploy",
            "title": "Example"
        },
        {
            "location": "/#push",
            "text": "",
            "title": "Push"
        },
        {
            "location": "/#name_4",
            "text": "Pushes a Docker image to AWS Elastic Container Service",
            "title": "Name"
        },
        {
            "location": "/#synopsis_5",
            "text": "sagify push [--aws-profile PROFILE_NAME] [--aws-region AWS_REGION] [--iam-role-arn IAM_ROLE] [--external-id EXTERNAL_ID]",
            "title": "Synopsis"
        },
        {
            "location": "/#description_5",
            "text": "This command pushes an already built Docker image to AWS Elastic Container Service. Later on, AWS SageMaker will consume that image from AWS Elastic Container Service for train and serve mode.   Only one of  iam-role-arn  and  aws_profile  can be provided.  external-id  is ignored when no  iam-role-arn  is provided.",
            "title": "Description"
        },
        {
            "location": "/#optional-flags_1",
            "text": "--iam-role-arn IAM_ROLE  or  -i IAM_ROLE : AWS IAM role to use for pushing to ECR  --aws-region AWS_REGION  or  -r AWS_REGION : The AWS region to push the image to  --aws-profile PROFILE_NAME  or  -p PROFILE_NAME : AWS profile to use for pushing to ECR  --external-id EXTERNAL_ID  or  -e EXTERNAL_ID : Optional external id used when using an IAM role",
            "title": "Optional Flags"
        },
        {
            "location": "/#example_5",
            "text": "sagify push",
            "title": "Example"
        },
        {
            "location": "/#cloud-upload-data",
            "text": "",
            "title": "Cloud Upload Data"
        },
        {
            "location": "/#name_5",
            "text": "Uploads data to AWS S3",
            "title": "Name"
        },
        {
            "location": "/#synopsis_6",
            "text": "sagify cloud upload-data --input-dir LOCAL_INPUT_DATA_DIR --s3-dir S3_TARGET_DATA_LOCATION",
            "title": "Synopsis"
        },
        {
            "location": "/#description_6",
            "text": "This command uploads content under  LOCAL_INPUT_DATA_DIR  to S3 under  S3_TARGET_DATA_LOCATION",
            "title": "Description"
        },
        {
            "location": "/#required-flags",
            "text": "--input-dir LOCAL_INPUT_DATA_DIR  or  -i LOCAL_INPUT_DATA_DIR : Local input directory  --s3-dir S3_TARGET_DATA_LOCATION  or  -s S3_TARGET_DATA_LOCATION : S3 target location",
            "title": "Required Flags"
        },
        {
            "location": "/#example_6",
            "text": "sagify cloud upload-data -i ./training_data/ -s s3://my-bucket/training-data/",
            "title": "Example"
        },
        {
            "location": "/#cloud-train",
            "text": "",
            "title": "Cloud Train"
        },
        {
            "location": "/#name_6",
            "text": "Trains your ML/DL model using a Docker image on AWS SageMaker with input from S3",
            "title": "Name"
        },
        {
            "location": "/#synopsis_7",
            "text": "sagify cloud train --input-s3-dir INPUT_DATA_S3_LOCATION --output-s3-dir S3_LOCATION_TO_SAVE_OUTPUT --ec2-type EC2_TYPE [--hyperparams-file HYPERPARAMS_JSON_FILE] [--volume-size EBS_SIZE_IN_GB] [--time-out TIME_OUT_IN_SECS] [--aws-tags TAGS] [--iam-role-arn IAM_ROLE] [--external-id EXTERNAL_ID] [--base-job-name BASE_JOB_NAME] [--job-name JOB_NAME] [--metric-names COMMA_SEPARATED_METRIC_NAMES]",
            "title": "Synopsis"
        },
        {
            "location": "/#description_7",
            "text": "This command retrieves a Docker image from AWS Elastic Container Service and executes it on AWS SageMaker in train mode",
            "title": "Description"
        },
        {
            "location": "/#required-flags_1",
            "text": "--input-s3-dir INPUT_DATA_S3_LOCATION  or  -i INPUT_DATA_S3_LOCATION : S3 location to input data  --output-s3-dir S3_LOCATION_TO_SAVE_OUTPUT  or  -o S3_LOCATION_TO_SAVE_OUTPUT : S3 location to save output (models, reports, etc). Make sure that the output bucket already exists. Any not existing key prefix will be created by sagify.  --ec2-type EC2_TYPE  or  -e EC2_TYPE : ec2 type. Refer to  https://aws.amazon.com/sagemaker/pricing/instance-types/",
            "title": "Required Flags"
        },
        {
            "location": "/#optional-flags_2",
            "text": "--hyperparams-file HYPERPARAMS_JSON_FILE  or  -h HYPERPARAMS_JSON_FILE : Path to hyperparams JSON file  --volume-size EBS_SIZE_IN_GB  or  -v EBS_SIZE_IN_GB : Size in GB of the EBS volume (default: 30)  --time-out TIME_OUT_IN_SECS  or  -s TIME_OUT_IN_SECS : Time-out in seconds (default: 24 * 60 * 60)  --aws-tags TAGS  or  -a TAGS : Tags for labeling a training job of the form  tag1=value1;tag2=value2 . For more, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.  --iam-role-arn IAM_ROLE  or  -r IAM_ROLE : AWS IAM role to use for training with  SageMaker  --external-id EXTERNAL_ID  or  -x EXTERNAL_ID : Optional external id used when using an IAM role  --base-job-name BASE_JOB_NAME  or  -n BASE_JOB_NAME : Optional prefix for the SageMaker training job  --job-name JOB_NAME : Optional name for the SageMaker training job. NOTE: if a  --base-job-name  is passed along with this option, it will be ignored.  --metric-names COMMA_SEPARATED_METRIC_NAMES : Optional comma-separated metric names for tracking performance of training jobs. Example:  Precision,Recall,AUC . Then, make sure you log these metric values using the  log_metric  function in the  train  function:  ```\n...\nfrom sagify.api.hyperparameter_tuning import log_metric\nlog_metric(\"Precision:, precision)\nlog_metric(\"Accuracy\", accuracy)\n...\n```  When the training jobs finishes, they will be stored in the CloudWatch algorithm metrics logs of the SageMaker training job:",
            "title": "Optional Flags"
        },
        {
            "location": "/#example_7",
            "text": "sagify cloud train -i s3://my-bucket/training-data/ -o s3://my-bucket/output/ -e ml.m4.xlarge -h local/path/to/hyperparams.json -v 60 -t 86400 --metric-names Accuracy,Precision",
            "title": "Example"
        },
        {
            "location": "/#cloud-hyperparameter-optimization",
            "text": "",
            "title": "Cloud Hyperparameter Optimization"
        },
        {
            "location": "/#name_7",
            "text": "Executes a Docker image in hyperparameter-optimization mode on AWS SageMaker",
            "title": "Name"
        },
        {
            "location": "/#synopsis_8",
            "text": "sagify cloud hyperparameter_optimization --input-s3-dir INPUT_DATA_S3_LOCATION --output-s3-dir S3_LOCATION_TO_SAVE_MULTIPLE_TRAINED_MODELS --ec2-type EC2_TYPE [--hyperparams-config-file HYPERPARAM_RANGES_JSON_FILE] [--max-jobs MAX_NUMBER_OF_TRAINING_JOBS] [--max-parallel-jobs MAX_NUMBER_OF_PARALLEL_TRAINING_JOBS] [--volume-size EBS_SIZE_IN_GB] [--time-out TIME_OUT_IN_SECS] [--aws-tags TAGS] [--iam-role-arn IAM_ROLE] [--external-id EXTERNAL_ID] [--base-job-name BASE_JOB_NAME] [--job-name JOB_NAME] [--wait WAIT_UNTIL_HYPERPARAM_JOB_IS_FINISHED]",
            "title": "Synopsis"
        },
        {
            "location": "/#description_8",
            "text": "This command retrieves a Docker image from AWS Elastic Container Service and executes it on AWS SageMaker in hyperparameter-optimization mode",
            "title": "Description"
        },
        {
            "location": "/#required-flags_2",
            "text": "--input-s3-dir INPUT_DATA_S3_LOCATION  or  -i INPUT_DATA_S3_LOCATION : S3 location to input data  --output-s3-dir S3_LOCATION_TO_SAVE_OUTPUT  or  -o S3_LOCATION_TO_SAVE_OUTPUT : S3 location to save output (models, reports, etc). Make sure that the output bucket already exists. Any not existing key prefix will be created by sagify.  --ec2-type EC2_TYPE  or  -e EC2_TYPE : ec2 type. Refer to  https://aws.amazon.com/sagemaker/pricing/instance-types/  --hyperparams-config-file HYPERPARAM_RANGES_JSON_FILE  or  -h HYPERPARAM_RANGES_JSON_FILE : Local path to hyperparameters configuration file. Example:  {\n    \"ParameterRanges\": {\n        \"CategoricalParameterRanges\": [\n            {\n                \"Name\": \"kernel\",\n                \"Values\": [\"linear\", \"rbf\"]\n            }\n        ],\n        \"ContinuousParameterRanges\": [\n        {\n          \"MinValue\": 0.001,\n          \"MaxValue\": 10,\n          \"Name\": \"gamma\"\n        }\n        ],\n        \"IntegerParameterRanges\": [\n            {\n                \"Name\": \"C\",\n                \"MinValue\": 1,\n                \"MaxValue\": 10\n            }\n        ]\n    },\n    \"ObjectiveMetric\": {\n        \"Name\": \"Precision\",\n        \"Type\": \"Maximize\"\n    }\n}",
            "title": "Required Flags"
        },
        {
            "location": "/#optional-flags_3",
            "text": "--max-jobs MAX_NUMBER_OF_TRAINING_JOBS  or  -m MAX_NUMBER_OF_TRAINING_JOBS : Maximum total number of training jobs to start for the hyperparameter tuning job (default: 3)  --max-parallel-jobs MAX_NUMBER_OF_PARALLEL_TRAINING_JOBS  or  -p MAX_NUMBER_OF_PARALLEL_TRAINING_JOBS : Maximum number of parallel training jobs to start (default: 1)  --volume-size EBS_SIZE_IN_GB  or  -v EBS_SIZE_IN_GB : Size in GB of the EBS volume (default: 30)  --time-out TIME_OUT_IN_SECS  or  -s TIME_OUT_IN_SECS : Time-out in seconds (default: 24 * 60 * 60)  --aws-tags TAGS  or  -a TAGS : Tags for labeling a training job of the form  tag1=value1;tag2=value2 . For more, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.  --iam-role-arn IAM_ROLE  or  -r IAM_ROLE : AWS IAM role to use for training with  SageMaker  --external-id EXTERNAL_ID  or  -x EXTERNAL_ID : Optional external id used when using an IAM role  --base-job-name BASE_JOB_NAME  or  -n BASE_JOB_NAME : Optional prefix for the SageMaker training job  --job-name JOB_NAME : Optional name for the SageMaker training job. NOTE: if a  --base-job-name  is passed along with this option, it will be ignored.   --wait WAIT_UNTIL_HYPERPARAM_JOB_IS_FINISHED  or  -w WAIT_UNTIL_HYPERPARAM_JOB_IS_FINISHED : Optional flag to wait until Hyperparameter Tuning is finished. (default: don't wait)",
            "title": "Optional Flags"
        },
        {
            "location": "/#example_8",
            "text": "sagify cloud hyperparameter_optimization -i s3://my-bucket/training-data/ -o s3://my-bucket/output/ -e ml.m4.xlarge -h local/path/to/hyperparam_ranges.json -v 60 -t 86400",
            "title": "Example"
        },
        {
            "location": "/#cloud-deploy",
            "text": "",
            "title": "Cloud Deploy"
        },
        {
            "location": "/#name_8",
            "text": "Executes a Docker image in serve mode on AWS SageMaker",
            "title": "Name"
        },
        {
            "location": "/#synopsis_9",
            "text": "sagify cloud deploy --s3-model-location S3_LOCATION_TO_MODEL_TAR_GZ --num-instance NUMBER_OF_EC2_INSTANCES --ec2-type EC2_TYPE [--aws-tags TAGS] [--iam-role-arn IAM_ROLE] [--external-id EXTERNAL_ID] [--endpoint-name ENDPOINT_NAME]",
            "title": "Synopsis"
        },
        {
            "location": "/#description_9",
            "text": "This command retrieves a Docker image from AWS Elastic Container Service and executes it on AWS SageMaker in serve mode. You can update an endpoint (model or number of instances) by specifying the endpoint-name.",
            "title": "Description"
        },
        {
            "location": "/#required-flags_3",
            "text": "--s3-model-location S3_LOCATION_TO_MODEL_TAR_GZ  or  -m S3_LOCATION_TO_MODEL_TAR_GZ : S3 location to to model tar.gz  --num-instances NUMBER_OF_EC2_INSTANCES  or  n NUMBER_OF_EC2_INSTANCES : Number of ec2 instances  --ec2-type EC2_TYPE  or  e EC2_TYPE : ec2 type. Refer to https://aws.amazon.com/sagemaker/pricing/instance-types/",
            "title": "Required Flags"
        },
        {
            "location": "/#optional-flags_4",
            "text": "--aws-tags TAGS  or  -a TAGS : Tags for labeling a training job of the form  tag1=value1;tag2=value2 . For more, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.  --iam-role-arn IAM_ROLE  or  -r IAM_ROLE : AWS IAM role to use for deploying with  SageMaker  --external-id EXTERNAL_ID  or  -x EXTERNAL_ID : Optional external id used when using an IAM role  --endpoint-name ENDPOINT_NAME : Optional name for the SageMaker endpoint",
            "title": "Optional Flags"
        },
        {
            "location": "/#example_9",
            "text": "sagify cloud deploy -m s3://my-bucket/output/model.tar.gz -n 3 -e ml.m4.xlarge",
            "title": "Example"
        },
        {
            "location": "/#cloud-batch-transform",
            "text": "",
            "title": "Cloud Batch Transform"
        },
        {
            "location": "/#name_9",
            "text": "Executes a Docker image in batch transform mode on AWS SageMaker, i.e. runs batch predictions on user defined S3 data",
            "title": "Name"
        },
        {
            "location": "/#synopsis_10",
            "text": "sagify cloud batch_transform --s3-model-location S3_LOCATION_TO_MODEL_TAR_GZ --s3-input-location S3_INPUT_LOCATION --s3-output-location S3_OUTPUT_LOCATION --num-instance NUMBER_OF_EC2_INSTANCES --ec2-type EC2_TYPE [--aws-tags TAGS] [--iam-role-arn IAM_ROLE] [--external-id EXTERNAL_ID] [--wait WAIT_UNTIL_BATCH_TRANSFORM_JOB_IS_FINISHED] [--job-name JOB_NAME]",
            "title": "Synopsis"
        },
        {
            "location": "/#description_10",
            "text": "This command retrieves a Docker image from AWS Elastic Container Service and executes it on AWS SageMaker in batch transform mode, i.e. runs batch predictions on user defined S3 data. SageMaker will spin up REST container(s) and call it/them with input data(features) from a user defined S3 path.  Things to do:\n- You should implement the predict function that expects a JSON containing the required feature values. It's the same predict function used for deploying the model as a REST service. Example of a JSON:  {\n    \"features\": [5.1,3.5,1.4,0.2]\n}   The input S3 path should contain a file or multiple files where each line is a JSON, the same JSON format as the one expected in the predict function. Example of a file:   {\"features\": [5.1,3.5,1.4,0.2]}\n{\"features\": [4.9,3.0,1.4,0.2]}\n{\"features\": [4.7,3.2,1.3,0.2]}\n{\"features\": [4.6,3.1,1.5,0.2]}",
            "title": "Description"
        },
        {
            "location": "/#required-flags_4",
            "text": "--s3-model-location S3_LOCATION_TO_MODEL_TAR_GZ  or  -m S3_LOCATION_TO_MODEL_TAR_GZ : S3 location to to model tar.gz  --s3-input-location S3_INPUT_LOCATION  or  -i S3_INPUT_LOCATION : s3 input data location  --s3-output-location S3_OUTPUT_LOCATION  or  -o S3_OUTPUT_LOCATION : s3 location to save predictions  --num-instances NUMBER_OF_EC2_INSTANCES  or  n NUMBER_OF_EC2_INSTANCES : Number of ec2 instances  --ec2-type EC2_TYPE  or  e EC2_TYPE : ec2 type. Refer to https://aws.amazon.com/sagemaker/pricing/instance-types/",
            "title": "Required Flags"
        },
        {
            "location": "/#optional-flags_5",
            "text": "--aws-tags TAGS  or  -a TAGS : Tags for labeling a training job of the form  tag1=value1;tag2=value2 . For more, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.  --iam-role-arn IAM_ROLE  or  -r IAM_ROLE : AWS IAM role to use for deploying with  SageMaker  --external-id EXTERNAL_ID  or  -x EXTERNAL_ID : Optional external id used when using an IAM role  --wait WAIT_UNTIL_BATCH_TRANSFORM_JOB_IS_FINISHED  or  -w WAIT_UNTIL_BATCH_TRANSFORM_JOB_IS_FINISHED : Optional flag to wait until Batch Transform is finished. (default: don't wait)  --job-name JOB_NAME : Optional name for the SageMaker batch transform job",
            "title": "Optional Flags"
        },
        {
            "location": "/#example_10",
            "text": "sagify cloud batch_transform -m s3://my-bucket/output/model.tar.gz -i s3://my-bucket/input_features -o s3://my-bucket/predictions -n 3 -e ml.m4.xlarge",
            "title": "Example"
        }
    ]
}